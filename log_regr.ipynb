{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Inmar - predicting flight delay\n",
    "This Python code is written to be run on Spark.  The model predicts whether a flight will be delayed by 15 minutes or more.  The independent variables are the max and min temperature reported on the day of the flight.\n",
    "\n",
    "A logistic regression algorithm was used because the outcome is binary.  I also thought about running standard regression model that would predict the amount of time delayed and then count the number of flights delayed past 15 minutes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##Plan for Deliverables\n",
    "The request from Inmar is to spin up three nodes using Horton Works's solution, create a model that predicts flights delayed by 15 minutes or greater, and write the code in iSpark (Scala programming language).\n",
    "\n",
    "As with most software projects the ideal can take some time.  Therefore, building on an Agile Methodology process, I broke the project down into deliverables.  The first is to write the code in Python.  This would allow working through the parsing of the data in a language in which I have delivered more often.  The follow on deliverables are 1- write the code in Scala and 2- spin up three nodes for the code to run on the full datasets."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##Sources\n",
    "In my research of logistic regression in MLlib, I came across [this blog](https://samarthbhargav.wordpress.com/2014/04/22/logistic-regression-in-apache-spark/) and [this Spark documentation](https://spark.apache.org/docs/1.3.0/mllib-linear-methods.html).  I borrowed their code and repurposed based on the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##Training and Test Data\n",
    "I used the 2007 sets as the training sets and the 2008 sets as the test sets.  A possible fourth deliverable could be to combine the sets and then create a 70/30 split."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from pyspark.mllib.regression import LabeledPoint, LinearRegressionWithSGD\n",
    "from pyspark.mllib.classification import LogisticRegressionWithSGD\n",
    "from pyspark.mllib.regression import LabeledPoint\n",
    "from numpy import array"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###Read data in from airline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def air_parse(x):\n",
    "    '''\n",
    "    air_parse takes in the line and parses the date and the delayed data point.\n",
    "    The date is created in the format of YYYYMMDD, which is consistent with the date in the weather data.\n",
    "    The delayed data point is 1 if greater than or equal to 15 and 0 otherwise. \n",
    "    '''\n",
    "    air_date = x[0]\n",
    "    # add leading zero if month is single digit\n",
    "    if len(x[1]) == 1:\n",
    "        air_date = air_date + '0'\n",
    "    air_date = air_date + x[1]\n",
    "        # add leading zero if month is single digit\n",
    "    if len(x[2]) == 1:\n",
    "        air_date = air_date + '0'\n",
    "    air_date = air_date + x[2]\n",
    "\n",
    "    # if delayed more than 15 minutes\n",
    "    if int(x[15]) >= 15:\n",
    "        air_delay_15 = 1\n",
    "    else:\n",
    "        air_delay_15 = 0\n",
    "\n",
    "    return([air_date, air_delay_15])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_air = sc.textFile('./data/airline/a2007.csv')\n",
    "\n",
    "# filter out header row\n",
    "# train_air = train_air.filter(lambda i: 'Year' not in i)\n",
    "# filter on rows that have 2007 - removes header and extra rows\n",
    "train_air = train_air.filter(lambda i: '2007' in i)\n",
    "\n",
    "# filter out canceled flights\n",
    "train_air = train_air.filter(lambda i: i.split(',')[21] == '0')\n",
    "    \n",
    "train_air = train_air.map(lambda i: i.split(',')).map(air_parse)\n",
    "#                           l.split(',')).map(lambda l: Rating(int(l[0]), int(l[1]), float(l[3]))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[u'20070101', 0],\n",
       " [u'20070101', 0],\n",
       " [u'20070101', 1],\n",
       " [u'20070101', 1],\n",
       " [u'20070102', 0],\n",
       " [u'20070102', 0],\n",
       " [u'20070102', 1],\n",
       " [u'20070102', 0],\n",
       " [u'20070103', 1],\n",
       " [u'20070103', 0]]"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_air.take(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###Read data from weather"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train_tempmax = sc.textFile('./data/weather/w2007.csv')\n",
    "\n",
    "# filter on TMAX and USW00094846\n",
    "train_tempmax = train_tempmax.filter(lambda i: 'TMAX' in i)\n",
    "train_tempmax = train_tempmax.filter(lambda i: 'USW00094846' in i)\n",
    "\n",
    "train_tempmax = train_tempmax.map(lambda i: i.split(',')).map(lambda i: [i[1], int(i[3])])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[u'20070101', -130],\n",
       " [u'20070102', 66],\n",
       " [u'20070103', 89],\n",
       " [u'20070104', 44],\n",
       " [u'20070105', -11],\n",
       " [u'20070106', 267],\n",
       " [u'20070107', 67],\n",
       " [u'20070108', -11],\n",
       " [u'20070109', -11]]"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_tempmax.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train_tempmin = sc.textFile('./data/weather/w2007.csv')\n",
    "\n",
    "# filter on TMAX and USW00094846\n",
    "train_tempmin = train_tempmin.filter(lambda i: 'TMIN' in i)\n",
    "train_tempmin = train_tempmin.filter(lambda i: 'USW00094846' in i)\n",
    "\n",
    "train_tempmin = train_tempmin.map(lambda i: i.split(',')).map(lambda i: [i[1], int(i[3])])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###Join the two data sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train_join_data = train_air.join(train_tempmax.join(train_tempmin))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###Create the LabeledPoints for building the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PythonRDD[1140] at RDD at PythonRDD.scala:43"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def mapper(line):\n",
    "    \"\"\"\n",
    "    mapper converts the joined datasets into a LabeledPoint - Label and Features\n",
    "    \"\"\"    \n",
    "    #### ***feats = line.strip().split(\",\") \n",
    "    # labels must be at the beginning for LRSGD, it's in the end in our data, so \n",
    "    # putting it in the right place\n",
    "    label = line[1][0]\n",
    "    feats = [line[1][1][0], line[1][1][1]]\n",
    "    feats.insert(0,label)\n",
    "    features = [ float(feature) for feature in feats ] # need floats\n",
    "    return(LabeledPoint(features[0], features[1:]))\n",
    "\n",
    "train_model_data = train_join_data.map(mapper)\n",
    "\n",
    "train_model_data.persist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###Train the data on 2007 data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Train model\n",
    "model = LogisticRegressionWithSGD.train(train_model_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###Parse 2008 data for testing the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###Airline data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test_air = sc.textFile('./data/airline/a2008.csv')\n",
    "\n",
    "# filter out header row\n",
    "# filter on rows that have 2007 - removes header and extra rows\n",
    "test_air = test_air.filter(lambda i: '2008' in i)\n",
    "\n",
    "# filter out canceled flights\n",
    "test_air = test_air.filter(lambda i: i.split(',')[21] == '0')\n",
    "    \n",
    "test_air = test_air.map(lambda i: i.split(',')).map(air_parse)\n",
    "#                           l.split(',')).map(lambda l: Rating(int(l[0]), int(l[1]), float(l[3]))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[u'20080103', 0],\n",
       " [u'20080103', 1],\n",
       " [u'20080103', 0],\n",
       " [u'20080103', 0],\n",
       " [u'20080104', 1],\n",
       " [u'20080104', 1],\n",
       " [u'20080104', 1],\n",
       " [u'20080104', 0],\n",
       " [u'20080105', 0],\n",
       " [u'20080105', 0]]"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_air.take(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###Weather Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test_tempmax = sc.textFile('./data/weather/w2008.csv')\n",
    "\n",
    "# filter on TMAX and USW00094846\n",
    "test_tempmax = test_tempmax.filter(lambda i: 'TMAX' in i)\n",
    "test_tempmax = test_tempmax.filter(lambda i: 'USW00094846' in i)\n",
    "\n",
    "test_tempmax = test_tempmax.map(lambda i: i.split(',')).map(lambda i: [i[1], int(i[3])])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test_tempmin = sc.textFile('./data/weather/w2008.csv')\n",
    "\n",
    "# filter on TMAX and USW00094846\n",
    "test_tempmin = test_tempmin.filter(lambda i: 'TMIN' in i)\n",
    "test_tempmin = test_tempmin.filter(lambda i: 'USW00094846' in i)\n",
    "\n",
    "test_tempmin = test_tempmin.map(lambda i: i.split(',')).map(lambda i: [i[1], int(i[3])])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test_join_data = test_air.join(test_tempmax.join(test_tempmin))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(u'20080107', (1, (-88, -214))),\n",
       " (u'20080107', (0, (-88, -214))),\n",
       " (u'20080107', (1, (-88, -214))),\n",
       " (u'20080107', (1, (-88, -214))),\n",
       " (u'20080106', (0, (22, -128))),\n",
       " (u'20080106', (0, (22, -128))),\n",
       " (u'20080106', (0, (22, -128))),\n",
       " (u'20080106', (0, (22, -128))),\n",
       " (u'20080105', (0, (278, 139))),\n",
       " (u'20080105', (0, (278, 139)))]"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_join_data.take(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###Create LabeledPoints for the calculation of error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test_model_data = test_join_data.map(mapper)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##Evaluate Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Error = 0.464285714286\n"
     ]
    }
   ],
   "source": [
    "# Evaluating the model on test data\n",
    "labels_preds = test_model_data.map(lambda p: (p.label, model.predict(p.features)))\n",
    "trainErr = labels_preds.filter(lambda (v, p): v != p).count() / float(test_model_data.count())\n",
    "print(\"Training Error = \" + str(trainErr))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
